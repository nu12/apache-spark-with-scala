{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab61be7",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "Uses SparkSession instead of SparkContext\n",
    "\n",
    "## DataFrames:\n",
    "* Contain rows.\n",
    "* Can run SQL queries.\n",
    "* Read and write JSON, Hive, parquet.\n",
    "* Communicate with JDBC/ODBC, Tableau.\n",
    "* Schema in inferred at runtime.\n",
    "\n",
    "## DataSets:\n",
    "* Can wrap a given struct or type (know up front what is inside it).\n",
    "* Schema can be inferred at compile time.\n",
    "* More efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbac60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://315284ee4037:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1625095554604)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "defined class Person\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "// Define a schema for our data\n",
    "case class Person(id:Int, name:String, age:Int, friends:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a95fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n",
      "[21,Miles,19,268]\n",
      "[52,Beverly,19,269]\n",
      "[54,Brunt,19,5]\n",
      "[106,Beverly,18,499]\n",
      "[115,Dukat,18,397]\n",
      "[133,Quark,19,265]\n",
      "[136,Will,19,335]\n",
      "[225,Elim,19,106]\n",
      "[304,Will,19,404]\n",
      "[341,Data,18,326]\n",
      "[366,Keiko,19,119]\n",
      "[373,Quark,19,272]\n",
      "[377,Beverly,18,418]\n",
      "[404,Kasidy,18,24]\n",
      "[409,Nog,19,267]\n",
      "[439,Data,18,417]\n",
      "[444,Keiko,18,472]\n",
      "[492,Dukat,19,36]\n",
      "[494,Kasidy,18,194]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schemaPeople: org.apache.spark.sql.Dataset[Person] = [id: int, name: string ... 2 more fields]\n",
       "teenagers: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n",
       "results: Array[org.apache.spark.sql.Row] = Array([21,Miles,19,268], [52,Beverly,19,269], [54,Brunt,19,5], [106,Beverly,18,499], [115,Dukat,18,397], [133,Quark,19,265], [136,Will,19,335], [225,Elim,19,106], [304,Will,19,404], [341,Data,18,326], [366,Keiko,19,119], [373,Quark,19,272], [377,Beverly,18,418], [404,Kasidy,18,24], [409,Nog,19,267], [439,Data,18,417], [444,Keiko,18,472], [492,Dukat,19,36], [494,Kasidy,18,194])\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Example using SQL\n",
    "\n",
    "val schemaPeople = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\",\"true\")\n",
    ".csv(\"data/fakefriends.csv\")\n",
    ".as[Person] //Takes the DataFrame and converts into a DataSet\n",
    "\n",
    "schemaPeople.printSchema()\n",
    "\n",
    "// Creates a temporary database that can be queried\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "val teenagers = spark.sql(\"SELECT * FROM people WHERE age <=19\")\n",
    "\n",
    "val results = teenagers.collect()\n",
    "\n",
    "results.foreach(println)\n",
    "\n",
    "//spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b1bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n",
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|    Will|\n",
      "|Jean-Luc|\n",
      "|    Hugh|\n",
      "|  Deanna|\n",
      "|   Quark|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+---+-------+\n",
      "| id|   name|age|friends|\n",
      "+---+-------+---+-------+\n",
      "| 21|  Miles| 19|    268|\n",
      "| 48|    Nog| 20|      1|\n",
      "| 52|Beverly| 19|    269|\n",
      "| 54|  Brunt| 19|      5|\n",
      "| 60| Geordi| 20|    100|\n",
      "+---+-------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 31|    8|\n",
      "| 65|    5|\n",
      "| 53|    7|\n",
      "| 34|    6|\n",
      "| 28|   10|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------+---+-------+---------+\n",
      "| id|    name|age|friends|newColumn|\n",
      "+---+--------+---+-------+---------+\n",
      "|  0|    Will| 33|    385|       43|\n",
      "|  1|Jean-Luc| 26|      2|       36|\n",
      "|  2|    Hugh| 55|    221|       65|\n",
      "|  3|  Deanna| 40|    465|       50|\n",
      "|  4|   Quark| 68|     21|       78|\n",
      "+---+--------+---+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "people: org.apache.spark.sql.Dataset[Person] = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Example using DataSet\n",
    "val people = schemaPeople\n",
    "\n",
    "people.printSchema()\n",
    "\n",
    "people.select(\"name\").show(5)\n",
    "\n",
    "people.filter(people(\"age\") < 21).show(5)\n",
    "\n",
    "people.groupBy(people(\"age\")).count().show(5)\n",
    "\n",
    "people.withColumn(\"newColumn\", people(\"age\") + 10).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "887693e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n",
      "+---+------------------+\n",
      "|age|      avg(friends)|\n",
      "+---+------------------+\n",
      "| 31|            267.25|\n",
      "| 65|             298.2|\n",
      "| 53|222.85714285714286|\n",
      "| 34|             245.5|\n",
      "| 28|             209.1|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "people: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Average friends by age\n",
    "\n",
    "val people = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"data/fakefriends.csv\")\n",
    "\n",
    "people.printSchema()\n",
    "\n",
    "people.select(\"age\", \"friends\").groupBy(\"age\").avg(\"friends\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5be5153f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Book\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Word counter example\n",
    "case class Book(value:String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "471982db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| you| 1878|\n",
      "|  to| 1828|\n",
      "|your| 1420|\n",
      "| the| 1292|\n",
      "|   a| 1191|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input: org.apache.spark.sql.Dataset[Book] = [value: string]\n",
       "words: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [word: string]\n",
       "lowerCaseWords: org.apache.spark.sql.DataFrame = [word: string]\n",
       "wordCountr: org.apache.spark.sql.DataFrame = [word: string, count: bigint]\n",
       "wordCountrSorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [word: string, count: bigint]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = spark.read.text(\"data/book.txt\").as[Book]\n",
    "\n",
    "val words = input\n",
    ".select(explode(split($\"value\", \"\\\\W+\")).alias(\"word\"))\n",
    ".filter($\"word\" =!= \"\")\n",
    "\n",
    "val lowerCaseWords = words.select(lower($\"word\").alias(\"word\"))\n",
    "\n",
    "val wordCountr = lowerCaseWords.groupBy(\"word\").count()\n",
    "\n",
    "val wordCountrSorted = wordCountr.sort(desc(\"count\"))\n",
    "\n",
    "wordCountrSorted.show(5) // show(wordCountrSorted.count.toInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbe6715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|  you| 1878|\n",
      "|   to| 1828|\n",
      "| your| 1420|\n",
      "|  the| 1292|\n",
      "|    a| 1191|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = data/book.txt MapPartitionsRDD[246] at textFile at <console>:33\n",
       "ds: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Making the example above easier using RDD + DS\n",
    "val rdd = sc.textFile(\"data/book.txt\")\n",
    "\n",
    "val ds = rdd.flatMap(x => x.split(\"\\\\W+\")).map(x => x.toLowerCase()).toDS()\n",
    "ds.groupBy(\"value\").count().sort(desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48ab37b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{FloatType, IntegerType, StringType, StructType}\n",
       "import org.apache.spark.sql.functions._\n",
       "defined class Temperature\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Min temperature example\n",
    "import org.apache.spark.sql.types.{FloatType, IntegerType, StringType, StructType}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "case class Temperature(stationID: String, date: Int, measure_type: String, temperature: Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49872014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----+----+\n",
      "|  stationID|min(temperature)|    C|   F|\n",
      "+-----------+----------------+-----+----+\n",
      "|ITE00100554|          -148.0|-14.8|5.36|\n",
      "|EZE00100082|          -135.0|-13.5| 7.7|\n",
      "+-----------+----------------+-----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "temperatureSchema: org.apache.spark.sql.types.StructType = StructType(StructField(stationID,StringType,true), StructField(date,IntegerType,true), StructField(measure_type,StringType,true), StructField(temperature,FloatType,true))\n",
       "ds: org.apache.spark.sql.Dataset[Temperature] = [stationID: string, date: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Specify the schema (instead of infer)\n",
    "val temperatureSchema = new StructType()\n",
    ".add(\"stationID\", StringType, nullable = true)\n",
    ".add(\"date\", IntegerType, nullable = true)\n",
    ".add(\"measure_type\", StringType, nullable = true)\n",
    ".add(\"temperature\", FloatType, nullable = true)\n",
    "\n",
    "val ds = spark.read\n",
    ".schema(temperatureSchema)\n",
    ".csv(\"data/1800.csv\")\n",
    ".as[Temperature]\n",
    "\n",
    "ds.filter($\"measure_type\" === \"TMIN\")\n",
    ".select(\"stationID\", \"temperature\")\n",
    ".groupBy(\"stationID\").min(\"temperature\")\n",
    ".withColumn(\"C\", $\"min(temperature)\" / 10)\n",
    ".withColumn(\"F\", round($\"C\" * 9 / 5 + 32, 2))\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30391680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|customerID|TotalSpent|\n",
      "+----------+----------+\n",
      "|        68|   6375.45|\n",
      "|        73|    6206.2|\n",
      "|        39|   6193.11|\n",
      "|        54|   6065.39|\n",
      "|        71|   5995.66|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.DataFrame = [customerID: int, itemID: int ... 1 more field]\n",
       "results: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [customerID: int, TotalSpent: double]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Total spent by customer\n",
    "\n",
    "val ds = spark.read\n",
    ".schema(\n",
    "    new StructType()\n",
    "    .add(\"customerID\", IntegerType)\n",
    "    .add(\"itemID\", IntegerType)\n",
    "    .add(\"price\", FloatType)\n",
    ")\n",
    ".csv(\"data/customer-orders.csv\")\n",
    "\n",
    "val results = ds\n",
    ".select(\"customerID\", \"price\")\n",
    ".groupBy(\"customerID\").agg(round(sum(\"price\"),2).alias(\"TotalSpent\"))\n",
    ".sort(desc(\"TotalSpent\"))\n",
    "\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d3cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
