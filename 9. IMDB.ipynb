{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971fc412",
   "metadata": {},
   "source": [
    "# IMDB\n",
    "\n",
    "This notebook processes IMDB data to find the following information:\n",
    "* The most connected actor.\n",
    "* Degrees of separation between two actors.\n",
    "\n",
    "To achieve the results, the process will have to perform the following tasks:\n",
    "* Read IMDB title princials file.\n",
    "* Map all lines to Title type (defined below) and reduce to one line per title.\n",
    "* Extract Actors/Actresses relationships from Title to create the Connection type.\n",
    "* Reduce Network to find the most connected Actor/Actress.\n",
    "* Perform BREADTH-FIRST SEARCH to find the (smallest) degrees of separation between two given Actors/Actresses. \n",
    "\n",
    "References:\n",
    "* [IMDB data description](https://www.imdb.com/interfaces/)\n",
    "* [IMDB datasets](https://datasets.imdbws.com/)\n",
    "* [BREADTH-FIRST SEARCH](https://en.wikipedia.org/wiki/Breadth-first_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b6980f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://78598ab614c0:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1625706066641)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.util.LongAccumulator\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.rdd.RDD\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "defined type alias Title\n",
       "defined type alias Connection\n",
       "defined type alias BFSData\n",
       "defined type alias BFSNode\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Imports and types definition\n",
    "\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "type Title = (String, Array[String]) // (Title id, list of actors/actresses)\n",
    "type Connection = (String, Array[String]) // (Actor/Actress id, connected actors/actresses)\n",
    "\n",
    "type BFSData = (Array[String], Int, Int) // (List of id, process status, separation)\n",
    "type BFSNode = (String, BFSData) // (Actor/Actress id, BFSData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d719dca",
   "metadata": {},
   "source": [
    "## First part: find the most connected acter/actress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37172b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tt6925244,nm0103977)\n",
      "(tt9472442,nm0352195)\n",
      "(tt9472442,nm0470050)\n",
      "(tt9472442,nm0021373)\n",
      "(tt9472442,nm0093210)\n",
      "(tt8374136,nm8045290)\n",
      "(tt8374136,nm8745061)\n",
      "(tt8374136,nm7329793)\n",
      "(tt6229652,nm7899734)\n",
      "(tt6229652,nm1699249)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "createRaw: (sc: org.apache.spark.SparkContext)org.apache.spark.rdd.RDD[(String, String)]\n",
       "raw: org.apache.spark.rdd.RDD[(String, String)] = SubtractedRDD[10] at subtractByKey at <console>:44\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read IMDB dataset, keep only actor and actress\n",
    "\n",
    "def createRaw(sc:SparkContext):RDD[(String,String)] = {\n",
    "    // Load base file\n",
    "    val base = sc.textFile(\"data/title.principals.tsv\")\n",
    "    .map(x => (x.split(\"\\t\")(0), x.split(\"\\t\")(2), x.split(\"\\t\")(3)))\n",
    "    .filter(x => x._3 == \"actor\" || x._3 == \"actress\")\n",
    "    .map(x => (x._1, x._2)) // Make pair RDD\n",
    "    \n",
    "    // Load titles to be removed (not a movie or isAdult = 1)\n",
    "    val adult = sc.textFile(\"data/title.basics.tsv\")\n",
    "    .map(x => (x.split(\"\\t\")(0),x.split(\"\\t\")(1),x.split(\"\\t\")(4)))\n",
    "    .filter(x => x._2 != \"movie\" || x._3 == 1)\n",
    "    .map(x => (x._1, 0)) // Make pair RDD\n",
    "    \n",
    "    return base.subtractByKey(adult)\n",
    "}\n",
    "\n",
    "val raw = createRaw(sc)\n",
    "\n",
    "raw.take(10).foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbac44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt5674064\n",
      "\tnm1794460\n",
      "\tnm0434263\n",
      "\tnm8105573\n",
      "\tnm0474609\n",
      "tt1620421\n",
      "\tnm1938461\n",
      "\tnm3310504\n",
      "\tnm2933317\n",
      "\tnm2714718\n",
      "tt1401235\n",
      "\tnm1543296\n",
      "\tnm0465631\n",
      "\tnm0619952\n",
      "\tnm2975382\n",
      "tt0125125\n",
      "\tnm0741411\n",
      "\tnm0214150\n",
      "\tnm0220116\n",
      "\tnm0222540\n",
      "\tnm0498140\n",
      "\tnm0277618\n",
      "\tnm0445173\n",
      "\tnm0446178\n",
      "\tnm0593722\n",
      "\tnm0000561\n",
      "tt0106810\n",
      "\tnm0293739\n",
      "\tnm0053903\n",
      "\tnm0433564\n",
      "\tnm0419951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "printTitle: (title: Title)Unit\n",
       "mapTitle: (line: (String, String))Title\n",
       "reduceTitle: (title1: Array[String], title2: Array[String])Array[String]\n",
       "titleRDD: org.apache.spark.rdd.RDD[(String, Array[String])] = ShuffledRDD[12] at reduceByKey at <console>:61\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Print to debug RDD\n",
    "def printTitle(title:Title){\n",
    "    println(title._1)\n",
    "    for(t <- title._2){\n",
    "        println(s\"\\t${t}\")\n",
    "    }\n",
    "}\n",
    "\n",
    "// Map the raw input to the Title type\n",
    "def mapTitle(line:(String, String)):Title = {\n",
    "    var arr:ArrayBuffer[String] = new ArrayBuffer()\n",
    "    arr += line._2\n",
    "    (line._1, arr.toArray)\n",
    "}\n",
    "\n",
    "// Recude the raw input to have one line poer title\n",
    "def reduceTitle(title1:Array[String], title2:Array[String]):Array[String] = {\n",
    "    var arr:ArrayBuffer[String] = new ArrayBuffer()\n",
    "    for(t <- title1){\n",
    "        arr += t\n",
    "    }\n",
    "    \n",
    "    for(t <- title2){\n",
    "        arr += t\n",
    "    }\n",
    "    \n",
    "    arr.toArray.distinct\n",
    "}\n",
    "\n",
    "val titleRDD = raw.map(mapTitle).reduceByKey(reduceTitle)\n",
    "\n",
    "titleRDD.take(5).foreach(printTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f057a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printConnection: (con: Connection)Unit\n",
       "titleToConnection: (title: Title)Array[Connection]\n",
       "reduceConnection: (c1: Array[String], c2: Array[String])Array[String]\n",
       "connectionRDD: org.apache.spark.rdd.RDD[(String, Array[String])] = ShuffledRDD[14] at reduceByKey at <console>:70\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Print to debug RDD\n",
    "def printConnection(con:Connection){\n",
    "    println(con._1)\n",
    "    for(c <- con._2){\n",
    "        println(s\"\\t${c}\")\n",
    "    }\n",
    "}\n",
    "\n",
    "def titleToConnection(title:Title):Array[Connection] = {\n",
    "    var outArray:ArrayBuffer[Connection] = new ArrayBuffer()\n",
    "    \n",
    "    val inArray = title._2\n",
    "    \n",
    "    for(k <- inArray){\n",
    "        var conArray:ArrayBuffer[String] = new ArrayBuffer()\n",
    "        for(v <- inArray){\n",
    "            if(k != v){ // Skip self\n",
    "                conArray += v\n",
    "            }\n",
    "        }\n",
    "        outArray += ((k, conArray.toArray))\n",
    "    }\n",
    "    outArray.toArray\n",
    "}\n",
    "\n",
    "def reduceConnection(c1:Array[String], c2:Array[String]): Array[String] = {\n",
    "    var outArray:ArrayBuffer[String] = new ArrayBuffer()\n",
    "    \n",
    "    for(c <- c1){\n",
    "        outArray += c\n",
    "    }\n",
    "    for(c <- c2){\n",
    "        outArray += c\n",
    "    }\n",
    "    outArray.toArray.distinct\n",
    "}\n",
    "\n",
    "val connectionRDD = titleRDD.flatMap(titleToConnection).reduceByKey(reduceConnection)\n",
    "\n",
    "//connectionRDD.take(5).foreach(printConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27163eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brahmanandam is the most connected actor/actress with 929 connections.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "printMostConnected: (sc: org.apache.spark.SparkContext, ac: (String, Int))Unit\n",
       "mostConnected: (String, Int) = (nm0103977,929)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printMostConnected(sc:SparkContext, ac:(String, Int)){\n",
    "    val nameLookup = sc.textFile(\"data/name.basics.tsv\")\n",
    "    .map(x => (x.split(\"\\t\")(0),x.split(\"\\t\")(1)))\n",
    "    .filter(x => x._1 == ac._1)\n",
    "    .take(1)(0)\n",
    "    \n",
    "    println(s\"${nameLookup._2} is the most connected actor/actress with ${ac._2} connections.\")\n",
    "}\n",
    "\n",
    "val mostConnected = connectionRDD.map(x => (x._1, x._2.length)).toDS().sort(desc(\"_2\")).take(1)(0)\n",
    "\n",
    "printMostConnected(sc, mostConnected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93cb0d",
   "metadata": {},
   "source": [
    "## Second part: perform BFS to find degrees of separation between to given actors/actresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadfb0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0. Processing 1 nodes.\n",
      "Iteration 1. Processing 0 nodes.\n",
      "Separation between nm3053338 and nm3053338 is 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "toBFSNode: (line: Connection)BFSNode\n",
       "bfsBase: org.apache.spark.rdd.RDD[BFSNode] = MapPartitionsRDD[24] at map at <console>:41\n",
       "fromAct: String = nm3053338\n",
       "toAct: String = nm3053338\n",
       "bfs: org.apache.spark.rdd.RDD[(String, (Array[String], Int, Int))] = ShuffledRDD[31] at reduceByKey at <console>:116\n",
       "hitCounter: Option[org.apache.spark.util.LongAccumulator] = Some(LongAccumulator(id: 213, name: Some(Hit Counter), value: 1))\n",
       "bfsMap: (node: BFSNode)Array[BFSNode]\n",
       "bfsReduce: (node1: BFSData, node2: BFSData)BFSData\n",
       "hitCounter: Option[org.apache.spark.util.LongAccumulator] = Some(LongAccumulator(id: 213, name: Some(Hit Counter), value: 1))\n",
       "iteration: Int = 2\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Process status => 0 = not processed / 1 = currently processing / 2 = already processed\n",
    "\n",
    "def toBFSNode(line: Connection):BFSNode = {\n",
    "    (line._1, (line._2, 0, 9999))\n",
    "}\n",
    "\n",
    "val bfsBase = connectionRDD.map(toBFSNode)\n",
    "\n",
    "// Select an actor/actress to start\n",
    "val fromAct = \"nm3053338\" // Robert De Niro\n",
    "// Select an actor/actress as target\n",
    "val toAct = \"nm3053338\" // Margot Robbie\n",
    "\n",
    "var bfs = bfsBase.map( x => if (x._1 == fromAct) (x._1, (x._2._1, 1, 0)) else x )\n",
    "\n",
    "// Accumulator\n",
    "var hitCounter:Option[LongAccumulator] = None\n",
    "\n",
    "// flatMap: explode nodes to be processed in next iteration\n",
    "def bfsMap(node:BFSNode):Array[BFSNode] = {\n",
    "    var outArray:ArrayBuffer[BFSNode] = new ArrayBuffer()\n",
    "    \n",
    "    var thisNode = node\n",
    "    \n",
    "    val id = node._1\n",
    "    val data = node._2\n",
    "    \n",
    "    val connections:Array[String] = data._1\n",
    "    val color = data._2\n",
    "    val currentDistance = data._3\n",
    "    \n",
    "    // If we find the target id, increase the count to stop the loop\n",
    "    if (id == toAct) {\n",
    "        if (hitCounter.isDefined) {\n",
    "            hitCounter.get.add(1)\n",
    "        }\n",
    "        outArray += ((id, (connections, 2, currentDistance + 1)))\n",
    "        return outArray.toArray\n",
    "    }\n",
    "    \n",
    "    if(color == 1){ // Gray node: This is the node we need to process\n",
    "        thisNode = (id, (connections, 2, currentDistance))\n",
    "        \n",
    "        // Add each connection to the output Array\n",
    "        for(c <- connections){\n",
    "            val emptyArray:Array[String] = new Array(0)\n",
    "            outArray += ((c, (emptyArray,1,currentDistance + 1)))\n",
    "        }\n",
    "    }\n",
    "    outArray += thisNode\n",
    "    outArray.toArray\n",
    "}\n",
    "\n",
    "\n",
    "// Reduce function: combine nodes in different states\n",
    "def bfsReduce(node1:BFSData, node2:BFSData):BFSData = {\n",
    "    val node1Connections = node1._1\n",
    "    val node2Connections = node2._1\n",
    "    val node1Color = node1._2\n",
    "    val node2Color = node2._2\n",
    "    val node1Distance = node1._3\n",
    "    val node2Distance = node2._3\n",
    "    \n",
    "    var outArray:ArrayBuffer[String] = new ArrayBuffer()\n",
    "    node1Connections.foreach( x => outArray += x)\n",
    "    node2Connections.foreach( x => outArray += x)\n",
    "\n",
    "    val outColor = List(node1Color, node2Color).max\n",
    "    val outDistance = List(node1Distance, node2Distance).min\n",
    "    \n",
    "    (outArray.toArray, outColor, outDistance)\n",
    "}\n",
    "\n",
    "hitCounter = Some(sc.longAccumulator(\"Hit Counter\"))\n",
    "var iteration:Int = 0\n",
    "while(hitCounter.get.value == 0){\n",
    "    val greyNodes = bfs.filter(x => x._2._2 == 1 )\n",
    "    println(s\"Iteration ${iteration}. Processing ${greyNodes.count()} nodes.\")\n",
    "    \n",
    "    val mapped = bfs.flatMap(bfsMap)\n",
    "    \n",
    "    bfs = mapped.reduceByKey(bfsReduce)\n",
    "    \n",
    "    iteration += 1\n",
    "}\n",
    "\n",
    "println(s\"Separation between ${fromAct} and ${toAct} is ${iteration - 1}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
