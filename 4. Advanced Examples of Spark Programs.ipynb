{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e4ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://78598ab614c0:4041\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1625707563495)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one unchecked warning; for details, enable `:setting -unchecked' or `:replay -unchecked'\n",
       "defined class Movie\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Most watched movie\n",
    "\n",
    "final case class Movie(userID: Int, movieID: Int, rating:Int,timestamp:Long )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c522cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userID|movieID|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|     3|881250949|\n",
      "|   186|    302|     3|891717742|\n",
      "|    22|    377|     1|878887116|\n",
      "|   244|     51|     2|880606923|\n",
      "|   166|    346|     1|886397596|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, IntegerType, LongType}\n",
       "ds: org.apache.spark.sql.Dataset[Movie] = [userID: int, movieID: int ... 2 more fields]\n",
       "topMovies: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [movieID: int, count: bigint]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, IntegerType, LongType}\n",
    "val ds = spark.read\n",
    ".option(\"sep\", \"\\t\")\n",
    ".schema(\n",
    "    new StructType()\n",
    "    .add(\"userID\", IntegerType, nullable= true)\n",
    "    .add(\"movieID\", IntegerType, nullable= true)\n",
    "    .add(\"rating\", IntegerType, nullable= true)\n",
    "    .add(\"timestamp\", LongType, nullable = true)\n",
    ")\n",
    ".csv(\"data/ml-100k/u.data\")\n",
    ".as[Movie]\n",
    "\n",
    "ds.show(5)\n",
    "\n",
    "val topMovies = ds.groupBy(\"movieID\").count().orderBy(desc(\"count\"))\n",
    "\n",
    "topMovies.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebed882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userID|movieID|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|     3|881250949|\n",
      "|   186|    302|     3|891717742|\n",
      "|    22|    377|     1|878887116|\n",
      "|   244|     51|     2|880606923|\n",
      "|   166|    346|     1|886397596|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "1682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import scala.io.{Codec, Source}\n",
       "import org.apache.spark.sql.functions.{col, udf}\n",
       "loadMovieNames: ()Map[Int,String]\n",
       "nameDict: org.apache.spark.broadcast.Broadcast[Map[Int,String]] = Broadcast(5)\n",
       "ds: org.apache.spark.sql.Dataset[Movie] = [userID: int, movieID: int ... 2 more fields]\n",
       "movieCounts: org.apache.spark.sql.DataFrame = [movieID: int, count: bigint]\n",
       "lookupName: Int => String = $Lambda$4162/0x0000000841684840@635d841\n",
       "lookupNameUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4162/0x0000000841684840@635d841,StringType,List(Some(class[value[0]: int])),Some(class[value[0]: string]),None,true,true)\n",
       "movieCountsWithName: org.apache.spark.sql.DataFrame = [movieID: int, count: bigint ... 1 more field]\n",
       "topMovies: org.apache.spark.sql.Dataset[org.a...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//-----------------------------------------------------------------\n",
    "// Join movie name: faslt lookup using in memory broadcasted map\n",
    "import scala.io.{Codec, Source}\n",
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "\n",
    "def loadMovieNames(): Map[Int, String] = {\n",
    "    // Codec of the file\n",
    "    implicit val codec = Codec(\"ISO-8859-1\")\n",
    "    \n",
    "    var movieNames:Map[Int,String] = Map()\n",
    "    \n",
    "    // Load the Map with the content of the file\n",
    "    val lines = Source.fromFile(\"data/ml-100k/u.item\")\n",
    "    for (line <- lines.getLines()){\n",
    "        val fields = line.split(\"|\")\n",
    "        if(fields.length > 1){\n",
    "            movieNames += (fields(0).toInt -> fields(1))\n",
    "        }\n",
    "    }\n",
    "    lines.close()\n",
    "    movieNames\n",
    "}\n",
    "\n",
    "val nameDict = sc.broadcast(loadMovieNames())\n",
    "\n",
    "val ds = spark.read\n",
    ".option(\"sep\", \"\\t\")\n",
    ".schema(\n",
    "    new StructType()\n",
    "    .add(\"userID\", IntegerType, nullable= true)\n",
    "    .add(\"movieID\", IntegerType, nullable= true)\n",
    "    .add(\"rating\", IntegerType, nullable= true)\n",
    "    .add(\"timestamp\", LongType, nullable = true)\n",
    ")\n",
    ".csv(\"data/ml-100k/u.data\")\n",
    ".as[Movie]\n",
    "\n",
    "ds.show(5)\n",
    "\n",
    "val movieCounts = ds.groupBy(\"movieID\").count()\n",
    "\n",
    "// User Defined Function\n",
    "val lookupName : Int => String = (movieId:Int) => {\n",
    "    nameDict.value(movieId)\n",
    "}\n",
    "\n",
    "// Wrap it with a UDF\n",
    "val lookupNameUDF = udf(lookupName)\n",
    "\n",
    "// Add movie title from the lookup table\n",
    "val movieCountsWithName = movieCounts.withColumn(\"movieTitle\",lookupNameUDF(col(\"movieId\")) )\n",
    "\n",
    "val topMovies = movieCountsWithName.sort(desc(\"count\"))\n",
    "\n",
    "//topMovies.show(5) // Exception =/\n",
    "println(topMovies.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2647fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MovieRating\n",
       "defined class MovieItem\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//-----------------------------------------------------------------\n",
    "// Let's try the above with joins\n",
    "case class MovieRating(movieID: Int)\n",
    "case class MovieItem(movieID: Int, movieTitle:String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|movieID|\n",
      "+-------+\n",
      "|    242|\n",
      "|    302|\n",
      "|    377|\n",
      "|     51|\n",
      "|    346|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-----------------+\n",
      "|movieID|       movieTitle|\n",
      "+-------+-----------------+\n",
      "|      1| Toy Story (1995)|\n",
      "|      2| GoldenEye (1995)|\n",
      "|      3|Four Rooms (1995)|\n",
      "|      4|Get Shorty (1995)|\n",
      "|      5|   Copycat (1995)|\n",
      "+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+------------+\n",
      "| ID|          Movie Name|# of Ratings|\n",
      "+---+--------------------+------------+\n",
      "| 50|    Star Wars (1977)|         583|\n",
      "|258|      Contact (1997)|         509|\n",
      "|100|        Fargo (1996)|         508|\n",
      "|181|Return of the Jed...|         507|\n",
      "|294|    Liar Liar (1997)|         485|\n",
      "|286|English Patient, ...|         481|\n",
      "|288|       Scream (1996)|         478|\n",
      "|  1|    Toy Story (1995)|         452|\n",
      "|300|Air Force One (1997)|         431|\n",
      "|121|Independence Day ...|         429|\n",
      "+---+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, IntegerType, LongType, StringType}\n",
       "ds: org.apache.spark.sql.Dataset[MovieRating] = [movieID: int]\n",
       "dsTitle: org.apache.spark.sql.Dataset[MovieItem] = [movieID: int, movieTitle: string]\n",
       "topMovies: org.apache.spark.sql.DataFrame = [ID: int, Movie Name: string ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, IntegerType, LongType, StringType}\n",
    "val ds = spark.read\n",
    ".option(\"sep\", \"\\t\")\n",
    ".schema(\n",
    "    new StructType()\n",
    "    .add(\"userID\", IntegerType, nullable= true)\n",
    "    .add(\"movieID\", IntegerType, nullable= true)\n",
    "    .add(\"rating\", IntegerType, nullable= true)\n",
    "    .add(\"timestamp\", LongType, nullable = true)\n",
    ")\n",
    ".csv(\"data/ml-100k/u.data\")\n",
    ".select(\"movieID\")\n",
    ".as[MovieRating]\n",
    "\n",
    "ds.show(5)\n",
    "\n",
    "val dsTitle = spark.read\n",
    ".option(\"sep\", \"|\")\n",
    ".schema(\n",
    "    new StructType()\n",
    "    .add(\"movieID\", IntegerType, nullable= true)\n",
    "    .add(\"movieTitle\", StringType, nullable= true)\n",
    "    .add(\"releaseDate\", StringType, nullable= true)\n",
    "    .add(\"empty\", StringType, nullable= true)\n",
    "    .add(\"url\", StringType, nullable= true)\n",
    ")\n",
    ".csv(\"data/ml-100k/u.item\")\n",
    ".select(\"movieID\", \"movieTitle\")\n",
    ".as[MovieItem]\n",
    "\n",
    "dsTitle.show(5)\n",
    "\n",
    "val topMovies = ds.groupBy(\"movieID\").count()\n",
    ".join(dsTitle, \"movieId\")\n",
    ".sort(desc(\"count\"))\n",
    ".select(\n",
    "    col(\"movieID\").alias(\"ID\"),\n",
    "    col(\"movieTitle\").alias(\"Movie Name\"),\n",
    "    col(\"count\").alias(\"# of Ratings\")\n",
    ")\n",
    "\n",
    "topMovies.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83da361",
   "metadata": {},
   "source": [
    "## Superhero Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c65e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class GraphLine\n",
       "defined class GraphName\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class GraphLine(line: String)\n",
    "case class GraphName(id: String, name: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5efb7bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------------------+\n",
      "|  id|totalConnections|                name|\n",
      "+----+----------------+--------------------+\n",
      "| 859|            1937|     CAPTAIN AMERICA|\n",
      "|5306|            1745|SPIDER-MAN/PETER PAR|\n",
      "|2664|            1532|IRON MAN/TONY STARK |\n",
      "|5716|            1429|THING/BENJAMIN J. GR|\n",
      "|6306|            1397|    WOLVERINE/LOGAN |\n",
      "+----+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CAPTAIN AMERICA is the most popular super hero with 1937 appearances.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StringType}\n",
       "raw: org.apache.spark.sql.Dataset[GraphLine] = [line: string]\n",
       "df: org.apache.spark.sql.DataFrame = [id: string, count: int]\n",
       "dfAgg: org.apache.spark.sql.DataFrame = [id: string, totalConnections: bigint]\n",
       "names: org.apache.spark.sql.Dataset[GraphName] = [id: string, name: string]\n",
       "popularity: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, totalConnections: bigint ... 1 more field]\n",
       "popularHero: org.apache.spark.sql.Row = [859,1937,CAPTAIN AMERICA]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Find the hero with most connections\n",
    "\n",
    "import org.apache.spark.sql.types.{StructType, StringType}\n",
    "    \n",
    "val raw = spark.read\n",
    ".schema(new StructType().add(\"line\", StringType, nullable = true))\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".text(\"data/Marvel-graph.txt\")\n",
    ".as[GraphLine]\n",
    "\n",
    "val df = raw\n",
    ".withColumn(\"id\", split(raw(\"line\"), \" \")(0))\n",
    ".withColumn(\"count\", size(split(raw(\"line\"), \" \")) - 1)\n",
    ".select(\"id\", \"count\")\n",
    "\n",
    "val dfAgg = df.groupBy(\"id\").agg(sum(\"count\").alias(\"totalConnections\"))\n",
    "\n",
    "val names = spark.read\n",
    ".schema(new StructType()\n",
    "        .add(\"id\", StringType, nullable=true)\n",
    "       .add(\"name\", StringType, nullable=true)\n",
    ")\n",
    ".option(\"sep\", \" \")\n",
    ".csv(\"data/Marvel-names.txt\")\n",
    ".as[GraphName]\n",
    "\n",
    "val popularity = dfAgg.join(names, \"id\").sort(desc(\"totalConnections\"))\n",
    "\n",
    "popularity.show(5)\n",
    "\n",
    "//println(names.filter($\"id\" === dfAgg.first()(0)).first().name)\n",
    "\n",
    "val popularHero = popularity.first()\n",
    "\n",
    "println(s\"${popularHero(2)} is the most popular super hero with ${popularHero(1)} appearances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e8790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTAIN AMERICA is the most popular super hero with 1937 appearances.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawRDD: org.apache.spark.rdd.RDD[String] = data/Marvel-graph.txt MapPartitionsRDD[71] at textFile at <console>:36\n",
       "processRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[72] at map at <console>:38\n",
       "reduceRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[73] at reduceByKey at <console>:40\n",
       "popularHeroRDDID: (Int, String) = (1937,859)\n",
       "popularHeroRDD: GraphName = GraphName(859,CAPTAIN AMERICA)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Do the same with RDD\n",
    "\n",
    "val rawRDD = sc.textFile(\"data/Marvel-graph.txt\")\n",
    "\n",
    "val processRDD = rawRDD.map( x => (x.split(\" \")(0), x.split(\" \").size))\n",
    "\n",
    "val reduceRDD = processRDD.reduceByKey( (acc, el) => acc + el )\n",
    "\n",
    "val popularHeroRDDID = reduceRDD.map( x => (x._2, x._1) ).max()\n",
    "\n",
    "val popularHeroRDD = names.filter($\"id\" === popularHeroRDDID._2).first()\n",
    "\n",
    "println(s\"${popularHeroRDD.name} is the most popular super hero with ${popularHeroRDDID._1} appearances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5d078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of obscure heroes - containing only 1 connection(s): \n",
      "BERSERKER II\n",
      "BLARE/\n",
      "MARVEL BOY II/MARTIN\n",
      "MARVEL BOY/MARTIN BU\n",
      "GIURESCU, RADU\n",
      "CLUMSY FOULUP\n",
      "FENRIS\n",
      "RANDAK\n",
      "SHARKSKIN\n",
      "CALLAHAN, DANNY\n",
      "DEATHCHARGE\n",
      "RUNE\n",
      "SEA LEOPARD\n",
      "RED WOLF II\n",
      "ZANTOR\n",
      "JOHNSON, LYNDON BAIN\n",
      "LUNATIK II\n",
      "KULL\n",
      "GERVASE, LADY ALYSSA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "popularityReversed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, totalConnections: bigint ... 1 more field]\n",
       "minConnections: Any = 1\n",
       "obscureHeroes: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, totalConnections: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Most \"obscure\" (heroes with less connections) hero\n",
    "\n",
    "val popularityReversed = popularity.sort(\"totalConnections\")\n",
    "\n",
    "val minConnections = popularityReversed.first()(1)\n",
    "\n",
    "val obscureHeroes = popularityReversed.where($\"totalConnections\" === minConnections)\n",
    "\n",
    "println(s\"List of obscure heroes - containing only ${minConnections} connection(s): \")\n",
    "\n",
    "obscureHeroes.foreach(x => println(x(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cdc68d",
   "metadata": {},
   "source": [
    "### BREADTH FIRST SEARCH to find degrees of separations\n",
    "![BFS](https://lh3.googleusercontent.com/proxy/hqz6ixlITI9tAQ11XQz71SFQG1DYAJruhALw0ORl2RhNSBQ1lW_JrfDJYzoPfdiEGty_IPGR8gNoarlmI-KHDCLYkDn0sd5PSvLDZJKQhiPsOTtKb-Bkod80EcUCDhU1Nn54gdTQ8AoF6-8k3nmvUpJNavRL4InJGLZn \"BFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6670c1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.util.LongAccumulator\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.rdd.RDD\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "startCharacterID: Int = 5306\n",
       "targetCharacterID: Int = 14\n",
       "hitCounter: Option[org.apache.spark.util.LongAccumulator] = None\n",
       "defined type alias BFSData\n",
       "defined type alias BFSNode\n",
       "convertToBFS: (line: String)BFSNode\n",
       "creatingStartingRdd: (sc: org.apache.spark.SparkContext)org.apache.spark.rdd.RDD[BFSNode]\n",
       "bfsMap: (node: BFSNode)Array[BFSNode]\n",
       "bfsReduce: (data1: BFSData, data2: BFSData)BFSData\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Degrees of separation: BREADTH-FIRST SEARCH\n",
    "\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "// The characters that we want to find the separation between\n",
    "val startCharacterID = 5306 // Spiderman\n",
    "val targetCharacterID = 14 // ADAM 3,031\n",
    "\n",
    "// Accumulator\n",
    "var hitCounter:Option[LongAccumulator] = None\n",
    "\n",
    "// Custom Data Types\n",
    "type BFSData = (Array[Int], Int, String)\n",
    "type BFSNode = (Int,BFSData)\n",
    "\n",
    "def convertToBFS(line:String):BFSNode = {\n",
    "    \n",
    "    val fields = line.split(\"\\\\s+\")\n",
    "    \n",
    "    val heroID = fields(0).toInt\n",
    "    \n",
    "    var connections: ArrayBuffer[Int] = ArrayBuffer()\n",
    "    for(connection <- 1 until (fields.length - 1)){\n",
    "        connections += fields(connection).toInt\n",
    "    }\n",
    "    \n",
    "    var color:String = \"WHITE\"\n",
    "    var distance:Int = 9999\n",
    "    \n",
    "    if(heroID == startCharacterID){\n",
    "        color = \"GRAY\"\n",
    "        distance = 0\n",
    "    }\n",
    "    \n",
    "    (heroID, (connections.toArray ,distance,color))\n",
    "}\n",
    "\n",
    "def creatingStartingRdd(sc:SparkContext):RDD[BFSNode] = {\n",
    "    val inputFile = sc.textFile(\"data/Marvel-graph.txt\")\n",
    "    inputFile.map(convertToBFS)\n",
    "}\n",
    "def bfsMap(node:BFSNode): Array[BFSNode] = {\n",
    "\n",
    "    // Extract data from the BFSNode\n",
    "    val characterID:Int = node._1\n",
    "    val data:BFSData = node._2\n",
    "\n",
    "    val connections:Array[Int] = data._1\n",
    "    val distance:Int = data._2\n",
    "    var color:String = data._3\n",
    "\n",
    "    // This is called from flatMap, so we return an array\n",
    "    // of potentially many BFSNodes to add to our new RDD\n",
    "    var results:ArrayBuffer[BFSNode] = ArrayBuffer()\n",
    "\n",
    "    // Gray nodes are flagged for expansion, and create new\n",
    "    // gray nodes for each connection\n",
    "    if (color == \"GRAY\") {\n",
    "        for (connection <- connections) {\n",
    "            val newCharacterID = connection\n",
    "            val newDistance = distance + 1\n",
    "            val newColor = \"GRAY\"\n",
    "\n",
    "            // Have we stumbled across the character we're looking for?\n",
    "            // If so increment our accumulator so the driver script knows.\n",
    "            if (targetCharacterID == connection) {\n",
    "                if (hitCounter.isDefined) {\n",
    "                    hitCounter.get.add(1)\n",
    "                }\n",
    "            }\n",
    "\n",
    "            // Create our new Gray node for this connection and add it to the results\n",
    "            val newEntry:BFSNode = (newCharacterID, (Array(), newDistance, newColor))\n",
    "            results += newEntry\n",
    "        }\n",
    "\n",
    "        // Color this node as black, indicating it has been processed already.\n",
    "        color = \"BLACK\"\n",
    "    }\n",
    "\n",
    "    // Add the original node back in, so its connections can get merged with \n",
    "    // the gray nodes in the reducer.\n",
    "    val thisEntry:BFSNode = (characterID, (connections, distance, color))\n",
    "    results += thisEntry\n",
    "\n",
    "    return results.toArray\n",
    "}\n",
    "\n",
    "/** Combine nodes for the same heroID, preserving the shortest length and darkest color. */\n",
    "def bfsReduce(data1:BFSData, data2:BFSData): BFSData = {\n",
    "\n",
    "    // Extract data that we are combining\n",
    "    val edges1:Array[Int] = data1._1\n",
    "    val edges2:Array[Int] = data2._1\n",
    "    val distance1:Int = data1._2\n",
    "    val distance2:Int = data2._2\n",
    "    val color1:String = data1._3\n",
    "    val color2:String = data2._3\n",
    "\n",
    "    // Default node values\n",
    "    var distance:Int = 9999\n",
    "    var color:String = \"WHITE\"\n",
    "    var edges:ArrayBuffer[Int] = ArrayBuffer()\n",
    "\n",
    "    // See if one is the original node with its connections.\n",
    "    // If so preserve them.\n",
    "    if (edges1.length > 0) {\n",
    "        edges ++= edges1\n",
    "    }\n",
    "    if (edges2.length > 0) {\n",
    "        edges ++= edges2\n",
    "    }\n",
    "\n",
    "    // Preserve minimum distance\n",
    "    if (distance1 < distance) {\n",
    "        distance = distance1\n",
    "    }\n",
    "    if (distance2 < distance) {\n",
    "        distance = distance2\n",
    "    }\n",
    "\n",
    "    // Preserve darkest color\n",
    "    if (color1 == \"WHITE\" && (color2 == \"GRAY\" || color2 == \"BLACK\")) {\n",
    "        color = color2\n",
    "    }\n",
    "    if (color1 == \"GRAY\" && color2 == \"BLACK\") {\n",
    "        color = color2\n",
    "    }\n",
    "    if (color2 == \"WHITE\" && (color1 == \"GRAY\" || color1 == \"BLACK\")) {\n",
    "        color = color1\n",
    "    }\n",
    "    if (color2 == \"GRAY\" && color1 == \"BLACK\") {\n",
    "        color = color1\n",
    "    }\n",
    "\n",
    "    return (edges.toArray, distance, color)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39f11f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFS Iteration# 1\n",
      "Processing 8326 values.\n",
      "Running BFS Iteration# 2\n",
      "Processing 218067 values.\n",
      "Hit the target character! From 1 different direction(s).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "run: ()Unit\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run(){\n",
    "   hitCounter = Some(sc.longAccumulator(\"Hit Counter\"))\n",
    "\n",
    "    var iterationRdd = creatingStartingRdd(sc)\n",
    "\n",
    "    var iteration:Int = 0\n",
    "    for (iteration <- 1 to 10) {\n",
    "      println(\"Running BFS Iteration# \" + iteration)\n",
    "\n",
    "      // Create new vertices as needed to darken or reduce distances in the\n",
    "      // reduce stage. If we encounter the node we're looking for as a GRAY\n",
    "      // node, increment our accumulator to signal that we're done.\n",
    "      val mapped = iterationRdd.flatMap(bfsMap)\n",
    "\n",
    "      // Note that mapped.count() action here forces the RDD to be evaluated, and\n",
    "      // that's the only reason our accumulator is actually updated.  \n",
    "      println(\"Processing \" + mapped.count() + \" values.\")\n",
    "\n",
    "      if (hitCounter.isDefined) {\n",
    "        val hitCount = hitCounter.get.value\n",
    "        if (hitCount > 0) {\n",
    "          println(\"Hit the target character! From \" + hitCount + \n",
    "              \" different direction(s).\")\n",
    "          return\n",
    "        }\n",
    "      }\n",
    "\n",
    "      // Reducer combines data for each character ID, preserving the darkest\n",
    "      // color and shortest path.      \n",
    "      iterationRdd = mapped.reduceByKey(bfsReduce)\n",
    "    } \n",
    "}\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e02099",
   "metadata": {},
   "source": [
    "## Item-based collaborative filtering\n",
    "\n",
    "Recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "331746ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.{Dataset, SparkSession}\n",
       "import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType}\n",
       "defined class Movies\n",
       "defined class MoviesNames\n",
       "defined class MoviePairs\n",
       "defined class MoviePairsSimilarity\n",
       "computeCosineSimilarity: (spark: org.apache.spark.sql.SparkSession, data: org.apache.spark.sql.Dataset[MoviePairs])org.apache.spark.sql.Dataset[MoviePairsSimilarity]\n",
       "getMovieName: (movieNames: org.apache.spark.sql.Dataset[MoviesNames], movieId: Int)String\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Movie similarity\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{Dataset, SparkSession}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType}\n",
    "\n",
    "case class Movies(userID: Int, movieID: Int, rating: Int, timestamp: Long)\n",
    "case class MoviesNames(movieID: Int, movieTitle: String)\n",
    "case class MoviePairs(movie1: Int, movie2: Int, rating1: Int, rating2: Int)\n",
    "case class MoviePairsSimilarity(movie1: Int, movie2: Int, score: Double, numPairs: Long)\n",
    "\n",
    "def computeCosineSimilarity(spark: SparkSession, data: Dataset[MoviePairs]): Dataset[MoviePairsSimilarity] = {\n",
    "    // Compute xx, xy and yy columns\n",
    "    val pairScores = data\n",
    "    .withColumn(\"xx\", col(\"rating1\") * col(\"rating1\"))\n",
    "    .withColumn(\"yy\", col(\"rating2\") * col(\"rating2\"))\n",
    "    .withColumn(\"xy\", col(\"rating1\") * col(\"rating2\"))\n",
    "\n",
    "    // Compute numerator, denominator and numPairs columns\n",
    "    val calculateSimilarity = pairScores\n",
    "    .groupBy(\"movie1\", \"movie2\")\n",
    "    .agg(\n",
    "    sum(col(\"xy\")).alias(\"numerator\"),\n",
    "    (sqrt(sum(col(\"xx\"))) * sqrt(sum(col(\"yy\")))).alias(\"denominator\"),\n",
    "    count(col(\"xy\")).alias(\"numPairs\")\n",
    "    )\n",
    "\n",
    "    // Calculate score and select only needed columns (movie1, movie2, score, numPairs)\n",
    "    import spark.implicits._\n",
    "    val result = calculateSimilarity\n",
    "    .withColumn(\"score\",\n",
    "    when(col(\"denominator\") =!= 0, col(\"numerator\") / col(\"denominator\"))\n",
    "    .otherwise(null)\n",
    "    ).select(\"movie1\", \"movie2\", \"score\", \"numPairs\").as[MoviePairsSimilarity]\n",
    "\n",
    "    result\n",
    "}\n",
    "\n",
    "/** Get movie name by given movie id */\n",
    "def getMovieName(movieNames: Dataset[MoviesNames], movieId: Int): String = {\n",
    "    val result = movieNames.filter(col(\"movieID\") === movieId)\n",
    "    .select(\"movieTitle\").collect()(0)\n",
    "\n",
    "    result(0).toString\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "087e4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading movie names...\n",
      "\n",
      "Top 10 similar movies for Pulp Fiction (1994)\n",
      "Smoke (1995)\tscore: 0.9743848338030823\tstrength: 68\n",
      "Reservoir Dogs (1992)\tscore: 0.9740674165782123\tstrength: 134\n",
      "Donnie Brasco (1997)\tscore: 0.9738247291149608\tstrength: 75\n",
      "Sling Blade (1996)\tscore: 0.9713796344244161\tstrength: 111\n",
      "True Romance (1993)\tscore: 0.9707295689679896\tstrength: 99\n",
      "Jackie Brown (1997)\tscore: 0.9706179145690377\tstrength: 55\n",
      "Carlito's Way (1993)\tscore: 0.9706021261759088\tstrength: 52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "runMovieSimilarity: (args: Array[String])Unit\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def runMovieSimilarity(args:Array[String]){\n",
    "    val moviesNamesSchema = new StructType()\n",
    "      .add(\"movieID\", IntegerType, nullable = true)\n",
    "      .add(\"movieTitle\", StringType, nullable = true)\n",
    "\n",
    "    // Create schema when reading u.data\n",
    "    val moviesSchema = new StructType()\n",
    "      .add(\"userID\", IntegerType, nullable = true)\n",
    "      .add(\"movieID\", IntegerType, nullable = true)\n",
    "      .add(\"rating\", IntegerType, nullable = true)\n",
    "      .add(\"timestamp\", LongType, nullable = true)\n",
    "\n",
    "    println(\"\\nLoading movie names...\")\n",
    "    import spark.implicits._\n",
    "    // Create a broadcast dataset of movieID and movieTitle.\n",
    "    // Apply ISO-885901 charset\n",
    "    val movieNames = spark.read\n",
    "      .option(\"sep\", \"|\")\n",
    "      .option(\"charset\", \"ISO-8859-1\")\n",
    "      .schema(moviesNamesSchema)\n",
    "      .csv(\"data/ml-100k/u.item\")\n",
    "      .as[MoviesNames]\n",
    "\n",
    "    // Load up movie data as dataset\n",
    "    val movies = spark.read\n",
    "      .option(\"sep\", \"\\t\")\n",
    "      .schema(moviesSchema)\n",
    "      .csv(\"data/ml-100k/u.data\")\n",
    "      .as[Movies]\n",
    "\n",
    "    val ratings = movies.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "    // Emit every movie rated together by the same user.\n",
    "    // Self-join to find every combination.\n",
    "    // Select movie pairs and rating pairs\n",
    "    val moviePairs = ratings.as(\"ratings1\")\n",
    "      .join(ratings.as(\"ratings2\"), $\"ratings1.userId\" === $\"ratings2.userId\" && $\"ratings1.movieId\" < $\"ratings2.movieId\")\n",
    "      .select($\"ratings1.movieId\".alias(\"movie1\"),\n",
    "        $\"ratings2.movieId\".alias(\"movie2\"),\n",
    "        $\"ratings1.rating\".alias(\"rating1\"),\n",
    "        $\"ratings2.rating\".alias(\"rating2\")\n",
    "      ).as[MoviePairs]\n",
    "\n",
    "    val moviePairSimilarities = computeCosineSimilarity(spark, moviePairs).cache()\n",
    "\n",
    "    if (args.length > 0) {\n",
    "      val scoreThreshold = 0.97\n",
    "      val coOccurrenceThreshold = 50.0\n",
    "\n",
    "      val movieID: Int = args(0).toInt\n",
    "\n",
    "      // Filter for movies with this sim that are \"good\" as defined by\n",
    "      // our quality thresholds above\n",
    "      val filteredResults = moviePairSimilarities.filter(\n",
    "        (col(\"movie1\") === movieID || col(\"movie2\") === movieID) &&\n",
    "          col(\"score\") > scoreThreshold && col(\"numPairs\") > coOccurrenceThreshold)\n",
    "\n",
    "      // Sort by quality score.\n",
    "      val results = filteredResults.sort(col(\"score\").desc).take(10)\n",
    "\n",
    "      println(\"\\nTop 10 similar movies for \" + getMovieName(movieNames, movieID))\n",
    "      for (result <- results) {\n",
    "        // Display the similarity result that isn't the movie we're looking at\n",
    "        var similarMovieID = result.movie1\n",
    "        if (similarMovieID == movieID) {\n",
    "          similarMovieID = result.movie2\n",
    "        }\n",
    "        println(getMovieName(movieNames, similarMovieID) + \"\\tscore: \" + result.score + \"\\tstrength: \" + result.numPairs)\n",
    "      }\n",
    "    }\n",
    " }\n",
    "\n",
    "runMovieSimilarity(Array(\"56\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
